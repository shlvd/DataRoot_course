{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"PCA.ipynb","provenance":[{"file_id":"1B9gvJL23ZgI6g8orrOYdR0jNQjEhvbVX","timestamp":1569961152752}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"mYv6Kmf0aaOu","colab_type":"text"},"source":["# Principal Component Analisis\n","\n","Welcome to your next lab! In this assignment you will explore problem of reducing dimensionality of data and restoring it back using your PCA model.\n","\n","\n","**You will learn to:**\n","- Build the general architecture of a learning algorithm with OOP in mind:\n","    - Helper functions\n","        - Normalization\n","        - Eigendecomposition\n","    - PCA Class\n","        - Transform\n","        - Restore"]},{"cell_type":"markdown","metadata":{"id":"xqGSGVTXaaOx","colab_type":"text"},"source":["## 1 - Packages ##\n","\n","First, let's run the cell below to import all the packages that you will need during this assignment. \n","- [numpy](www.numpy.org) is the fundamental package for scientific computing with Python.\n","- [matplotlib](http://matplotlib.org) is a famous library to plot graphs in Python.\n","- [PIL](https://pypi.org/project/PIL/) is the Python Imaging Library.\n","- [mpl_toolkits](https://matplotlib.org/mpl_toolkits/index.html) collections of application-specific functions that extend Matplotlib."]},{"cell_type":"code","metadata":{"id":"caEUwSVIaaOy","colab_type":"code","colab":{}},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","\n","from mpl_toolkits.mplot3d import Axes3D\n","from mpl_toolkits.mplot3d import proj3d"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WT1gorpfaaO6","colab_type":"text"},"source":["## 2 - Overview of the Problem set ##\n","\n","**Problem Statement**: \n","In this assignment we have an image as dataset. We will reduce its dimensionality using our algorithm. Here it is represented as array with dimension (1280, 960)."]},{"cell_type":"code","metadata":{"id":"G1AgYQ8gaaO8","colab_type":"code","colab":{}},"source":["img = Image.open('cat2.jpeg')\n","img = img.convert('L', colors=256)\n","img = np.array(img, dtype=np.uint8)\n","imgplot = plt.imshow(img, cmap='gray')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"E1pvBRfoaaPC","colab_type":"code","colab":{}},"source":["print('Shape of image:', img.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hWk35GejaaPH","colab_type":"text"},"source":["**Expected Output**: \n","\n","Shape of image: (1280, 960)"]},{"cell_type":"markdown","metadata":{"id":"r7vkukJ3aaPK","colab_type":"text"},"source":["But firstly we will check performance of PCA on simple synthetic dataset, which includes 13 examples with 3 features:"]},{"cell_type":"code","metadata":{"id":"6bO68NN_aaPL","colab_type":"code","colab":{}},"source":["X = np.asarray([[12, 15, 20, 24, 27, 30, 63, 8, 67, 43, 11, 15, 67], \n","                [34, 31, 29, 88, 76, 80, 89, 53, 48, 66, 45, 50, 85], \n","                [45, 50, 43, 60, 65, 59, 89, 53, 43, 31, 33, 40, 80]]).T\n","print('Shape of input data:', X.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sd6IJIvUaaPS","colab_type":"text"},"source":["**Expected output:**\n","    \n","Shape of input data: (13, 3)"]},{"cell_type":"markdown","metadata":{"id":"FVEWrxh0aaPT","colab_type":"text"},"source":["Let's visualize it:\n"]},{"cell_type":"code","metadata":{"id":"jwZVMqTSaaPW","colab_type":"code","colab":{}},"source":["fig = plt.figure(figsize=(8,8))\n","ax = fig.add_subplot(111, projection='3d')\n","plt.rcParams['legend.fontsize'] = 10   \n","ax.plot(X.T[0,:], X.T[1,:], X.T[2,:], 'o', markersize=8, color='blue', alpha=0.5, label='Initial')\n","\n","ax.legend(loc='upper right')\n","\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LM61ZQPvaaPc","colab_type":"text"},"source":["## 3 -  Principal Component Analisis. General Architecture\n","**Mathematical expression of the algorithm**:\n","\n","Principal Component Analysis (PCA) is a simple linear transformation technique that is used in numerous applications, such as stock market predictions, the analysis of gene expression data, and many more. \n","\n","Steps of the algorithm:\n","- Standardize the data;\n","- Obtain the Eigenvectors and Eigenvalues from the covariance matrix;\n","- Sort eigenvalues in descending order and choose the $k$ eigenvectors that correspond to the $k$ largest eigenvalues where $k$ is the number of dimensions of the new feature subspace $(k≤$n_features);\n","- Construct the projection matrix $W$ from the selected $k$ eigenvectors;\n","- Transform the original dataset $X$ via $W$ to obtain a $k$-dimensional feature subspace $Y$."]},{"cell_type":"markdown","metadata":{"id":"cdhebXYbaaPf","colab_type":"text"},"source":["### 3.1 -  Eigendecomposition: Computing Eigenvectors and Eigenvalues, \n","\n","**Standardizing the data**\n","\n","Whether to standardize the data prior to a PCA on the covariance matrix depends on the measurement scales of the original features. Since PCA yields a feature subspace that maximizes the variance along the axes, it makes sense to standardize the data, especially, if it was measured on different scales. \n","\n","Let's transform the data onto unit scale, which is a requirement for the optimal performance of many machine learning algorithms. We will do it using standard score:\n","$$z = \\frac{x - \\mu}{\\sigma}\\tag{1}$$"]},{"cell_type":"code","metadata":{"id":"0ZxJqj47aaPg","colab_type":"code","colab":{}},"source":["# GRADED FUNCTION: normalize\n","\n","def normalize(X):\n","    \"\"\"\n","    Normalise data before processing\n","    \n","    Arguments:\n","    X -- matrix of input features of shape (n_examples, n_features)\n","    \n","    Returns:\n","    X_norm -- normalized matrix of shape (n_examples, n_features)\n","    norm_parameters -- matrix of normalization parameters, such as mean and std by columns;\n","        shape (2, n_features)\n","        \n","    \"\"\"\n","    \n","    ### START CODE HERE ### \n","    std = np.std(X, axis=0)\n","    res = []\n","    mean = np.mean(X, axis=0)\n","    for row in X:\n","        res.append ( (row - mean )/ std)\n","    X_norm = np.array(res)\n","    norm_parameters = np.stack((mean,std))\n","    ### END CODE HERE ###\n","\n","    return X_norm, norm_parameters"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3mIontZ7aaPn","colab_type":"code","colab":{}},"source":["norm_test, norm_pars_test = normalize(X)\n","print('Normalized:')\n","print(norm_test[0])\n","print('Parameters:')\n","print(norm_pars_test)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AWQFoMvlaaPy","colab_type":"text"},"source":["**Expected Output**: \n","\n","<table>\n","    <tr>\n","        <td> **Normalized: ** </td>\n","       <td> [-0.89985013 -1.19521931 -0.49137306]</td>\n","        \n","    </tr>\n","    \n","    <tr>\n","        <td> **Parameters:** </td>\n","       <td>[[30.92307692 59.53846154 53.15384615]\n"," [21.02914292 21.36717607 16.59400334]]</td>\n","        \n","    </tr>\n","    \n","\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"_UN07P2jaaP2","colab_type":"text"},"source":["The eigenvectors and eigenvalues of a covariance (or correlation) matrix represent the \"core\" of a PCA: The eigenvectors (principal components) determine the directions of the new feature space, and the eigenvalues determine their magnitude. In other words, the eigenvalues explain the variance of the data along the new feature axes.\n","\n","**Covariance Matrix**\n","\n","The classic approach to PCA is to perform the eigendecomposition on the covariance matrix $Σ$, which is a $n_{features}×n_{features}$ matrix where each element represents the covariance between two features.\n","\n","We can summarize the calculation of the covariance matrix via the following matrix equation:\n","$$\\Sigma_{ij} = \\frac{1}{N} \\sum_{n=1}^{N} (x(n)_i – {\\bar{x}}_i)(x(n)_j – {\\bar{x}}_j)\\tag{2}$$\n","where $\\bar{x}$ is the mean vector ${\\bar{x}} = \\sum\\limits_{k=1}^n x_{k}$.\n","The mean vector is a $n_{features}$-dimensional vector where each value in this vector represents the sample mean of a feature column in the dataset.\n","\n","\n","**Correlation Matrix**\n","\n","A correlation matrix is used if the variances of individual variates differ much, or if the units of measurement of the individual variates differ.\n","\n","$$\\Sigma_{ij} = \\frac{c_{ij}}{\\sqrt{c_{ii}c_{jj}}},\\tag{3}$$\n","where $x$ - elements of covariation matrix.\n","\n","\n","**SVD**\n","\n","While the eigendecomposition of the covariance or correlation matrix may be more intuitiuve, most PCA implementations perform a Singular Vector Decomposition (SVD) to improve the computational efficiency. \n","\n","To perform SVD in the right way we need to transpose the input matrix, so $X^T$ will have dimension $n_{features}×n_{samples}$. Let us assume that it is centered, i.e. column means have been subtracted and are now equal to zero.\n","\n","Then the $n_{features}×n_{features}$ covariance matrix $C$ is given by $$C=X^TX/(n_{samples}−1).\\tag{4}$$ It is a symmetric matrix and so it can be diagonalized: $$C = ULU^T,\\tag{5}$$\n","where $U$ is a matrix of eigenvectors (each column is an eigenvector) and $L$ is a diagonal matrix with eigenvalues $λ_i$ in the decreasing order on the diagonal. The eigenvectors are called principal axes or principal directions of the data. Projections of the data on the principal axes are called principal components, also known as PC scores; these can be seen as new, transformed, variables. The $j$-th principal component is given by $j$-th column of $X^TV$. The coordinates of the $i$-th data point in the new PC space are given by the $i$-th row of $UX^T$.\n","\n","If we now perform singular value decomposition of $X^T$, we obtain a decomposition\n","$$X^T=USV^T,\\tag{6}$$\n","where $U$ is a unitary matrix and $S$ is the diagonal matrix of singular values $s_i$. From here one can easily see that $$C = USV^TVSU^T/(n_{samples}-1) = U \\frac{S^2}{n_{samples}-1}U^T,\\tag{7}$$\n","meaning that right singular vectors $U$ are principal directions and that singular values are related to the eigenvalues of covariance matrix via $λ_i=s^2_i/(n_{samples}−1)\\tag{8}$."]},{"cell_type":"code","metadata":{"id":"ZxHUD411aaP4","colab_type":"code","colab":{}},"source":["# GRADED CLASS: Eigendecomposition\n","\n","class Eigendecomposition():\n","        \n","    def covariance(self, X):\n","        \"\"\"\n","        Calculates eigenvectors and eigenvalues of covariance matrix\n","        Arguments:\n","        \n","        X -- normalized input matrix with shape (n_examples, n_features) \n","        \n","        Returnes:\n","        \n","        eig_vals -- array of eigenvalues with shape (n_features, )\n","        eig_vecs -- matrix of eigenvectors with shape (n_features, n_features)\n","        \"\"\"\n","        \n","        ### START CODE HERE ###\n","        cv = np.cov(np.transpose(X))\n","        w, v = np.linalg.eig(cv)\n","        e_val = w\n","        e_vect = v\n","        ### END CODE HERE ###\n","        return e_val, e_vect\n","    \n","    def correlation(self, X):\n","        \"\"\"\n","        Calculates eigenvectors and eigenvalues of correlation matrix\n","        Arguments:\n","        \n","        X -- normalized input matrix with shape (n_examples, n_features) \n","        \n","        Returnes:\n","        \n","        eig_vals -- array of eigenvalues with shape (n_features, )\n","        eig_vecs -- matrix of eigenvectors with shape (n_features, n_features)\n","        \"\"\"\n","        \n","        ### START CODE HERE ###\n","        cor = np.corrcoef(np.transpose(X))\n","        w, v = np.linalg.eig(cor)\n","        e_val = w\n","        e_vect = v\n","        ### END CODE HERE ###\n","        return e_val, e_vect\n","    \n","    def svd(self, X):\n","        \"\"\"\n","        Calculates eigenvectors and eigenvalues by svd\n","        Arguments:\n","        \n","        X -- normalized input matrix with shape (n_examples, n_features)\n","        \n","        Returnes:\n","        \n","        eig_vals -- array of eigenvalues with shape (n_features, )\n","        eig_vecs -- matrix of eigenvectors with shape (n_features, n_features)\n","        \"\"\"\n","        \n","        ### START CODE HERE ###\n","        e_vect, e_val, vh = np.linalg.svd(np.transpose(X), full_matrices=True)\n","        e_val = (e_val**2) / (X.shape[0]-1) \n","        ### END CODE HERE ###\n","        return e_val, e_vect\n","        "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1wvKwcXMaaQE","colab_type":"code","colab":{}},"source":["cov_vals, cov_vects = Eigendecomposition().covariance(norm_test)\n","\n","for ev in cov_vects:\n","    np.testing.assert_array_almost_equal(1.0, np.linalg.norm(ev))\n","print('Everything ok!')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MSRrHUP9aaQN","colab_type":"code","colab":{}},"source":["cor_vals, cor_vects = Eigendecomposition().correlation(norm_test)\n","\n","for ev in cor_vects:\n","    np.testing.assert_array_almost_equal(1.0, np.linalg.norm(ev))\n","print('Everything ok!')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"z1VS00guaaQd","colab_type":"code","colab":{}},"source":["svd_vals, svd_vects = Eigendecomposition().svd(norm_test)\n","\n","for ev in svd_vects:\n","    np.testing.assert_array_almost_equal(1.0, np.linalg.norm(ev))\n","print('Everything ok!')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"y5JS154SaaQq","colab_type":"text"},"source":["### 3.2-3.3 - Selecting Principal Components, Projection Onto the New Feature Space\n","\n","The typical goal of a PCA is to reduce the dimensionality of the original feature space by projecting it onto a smaller subspace, where the eigenvectors will form the axes. \n","\n","In order to decide which eigenvector(s) can be dropped without losing too much information for the construction of lower-dimensional subspace, we need to inspect the corresponding eigenvalues: the eigenvectors with the lowest eigenvalues bear the least information about the distribution of the data - those are the ones can be dropped.\n","\n","So now we need to implement following steps:\n","1. Compute eigenvectors $(e_1,e_2,...,e_d)$ and corresponding eigenvalues $(λ_1,λ_2,...,λ_d)$.\n","In order to do so, the common approach is to rank the eigenvalues from highest to lowest in order choose the top $k$ eigenvectors.\n","2. Sort the eigenvectors by decreasing eigenvalues and choose $k$ eigenvectors with the largest eigenvalues to form a $n_{features}×k$ dimensional matrix $W$ (where every row represents an eigenvector)\n","3. Use this $n_{features}×k$ eigenvector matrix to transform the samples onto the new subspace. This can be summarized by the mathematical equation: $$Y=X_{normalized}×W\\tag{9}$$ (where $X_{normalized}$ is an normalized input matrix with shape $n_{samples}×n_{features}$, and $Y$ is the transformed $n_{samples}×k$-dimensional matrix of samples in the new subspace.)\n","\n","4. To restore input matrix $X$ we should do next operation: $$X_{restored} = Y×W^T.\\tag{10}$$ But don't forget that we worked with normalized data, so we also have to put it to the initial form back (use `norm_parameters` from `normalization` function)."]},{"cell_type":"code","metadata":{"id":"04kV_rK3aaQs","colab_type":"code","colab":{}},"source":["# GRADED CLASS: PCA\n","\n","class PCA():\n","    \"\"\"\n","    Parameters:\n","    -----------\n","    X -- matrix of input features of shape (n_examples, n_features) \n","    n -- number of principal components\n","    \"\"\"\n","    \n","    def __init__(self, X, n, eigendecomposition):\n","        self.X = X\n","        self.n = n\n","        self.eigendecomposition = eigendecomposition\n","        self.X_norm = None\n","        self.norm_params = None\n","        \n","        \n","    def transform(self):\n","        \"\"\"\n","        Transforms the samples into the new subspace\n","        \n","        Returns:\n","        transformed -- reduced matrix of input features with shape (n_samples, n)\n","        matrix_w -- matrix of n eigenvectors with largest eigenvalues with shape (n_features, n)\n","        \"\"\"\n","        \n","        ### START CODE HERE ###\n","        self.X_norm, self.norm_params = normalize(self.X)\n","        eig_vals, eig_vecs = self.eigendecomposition(self.X_norm)\n","        eig_pairs = list([(eig_vals[i], eig_vecs[:,i]) for i in range(len(eig_vals))])\n","        eig_pairs.sort()\n","        eig_pairs.reverse()\n","        matrix_w = []\n","        for i in eig_pairs[:self.n]:\n","              matrix_w.append(i[1])\n","        matrix_w = np.array(matrix_w)\n","        matrix_w = matrix_w.T\n","        transformed = np.dot(self.X_norm,matrix_w)\n","        ### END CODE HERE ###\n","\n","        return transformed, matrix_w\n","\n","        \n","    def restore(self):\n","        \"\"\"\n","        Restores \"original\" values\n","\n","        Returns:\n","        matrix with shape (n_features, n_examples), \n","            containing restored values depending on reduced input matrix of features   \n","        \"\"\"\n","        \n","        ### START CODE HERE ###\n","        transformed, matrix_w = self.transform()\n","        X_r = np.dot(transformed,matrix_w.T)\n","        X_rest = X_r* self.norm_params[1]+ self.norm_params[0]\n","        return X_rest\n","        ### END CODE HERE ###"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WRMghyQ8aaQ0","colab_type":"text"},"source":["After completing all steps, the next question is “how many principal components are we going to choose for our new feature subspace?” A useful measure is the so-called “explained variance,” which can be calculated from the eigenvalues. The explained variance tells us how much information (variance) can be attributed to each of the principal components."]},{"cell_type":"code","metadata":{"id":"mDyCNuCXaaQ2","colab_type":"code","colab":{}},"source":["tot = sum(cov_vals)\n","var_exp = [(i / tot)*100 for i in sorted(cov_vals, reverse=True)]\n","cum_var_exp = np.cumsum(var_exp)\n","\n","with plt.style.context('seaborn-whitegrid'):\n","    plt.figure(figsize=(6, 4))\n","\n","    plt.bar(range(3), var_exp, alpha=0.5, align='center',\n","            label='individual explained variance')\n","    plt.step(range(3), cum_var_exp, where='mid',\n","             label='cumulative explained variance')\n","    plt.ylabel('Explained variance ratio')\n","    plt.xlabel('Principal components')\n","    plt.legend(loc='best')\n","    plt.tight_layout()\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vS_uUQMGaaQ9","colab_type":"text"},"source":["The plot above clearly shows that most of the variance (~77% of the variance to be precise) can be explained by the first principal component alone. The second principal component still bears some information (~19%) while the third and fourth principal components can safely be dropped without losing to much information. Together, the first two principal components contain ~96% of the information.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ynEAoZ5waaRD","colab_type":"text"},"source":["Initialize model:"]},{"cell_type":"code","metadata":{"id":"5lQqOjCaaaRE","colab_type":"code","colab":{}},"source":["pca = PCA(X, 2, Eigendecomposition().correlation)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4qxSCzoqaaRJ","colab_type":"text"},"source":["Reducing of input matrix dimencion:"]},{"cell_type":"code","metadata":{"id":"iyoR8Iz3aaRK","colab_type":"code","colab":{}},"source":["reduced_x, reduced_eigenvects = pca.transform()\n","print('Reduced input matrix:')\n","print(reduced_x[:5])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PuptB9EfaaRT","colab_type":"text"},"source":["**Expected Output**: \n","\n","<table>\n","    <tr>\n","       <td>\n","        Reduced input matrix:\n","       </td>\n","       <td> [[-1.49086967 -0.15159675]\n"," [-1.3199115  -0.10840709]\n"," [-1.50049321  0.29895991]\n"," [ 0.87097958 -0.90026658]\n"," [ 0.7886636  -0.71439948]]\n","    </tr>\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"-ueVCiI3aaRV","colab_type":"text"},"source":["Let's plot our reduced data and compare it with original:"]},{"cell_type":"code","metadata":{"id":"G4yIZtwmaaRY","colab_type":"code","colab":{}},"source":["plt.plot(reduced_x.T[0,:], reduced_x.T[1,:], 'o', markersize=7, color='blue', alpha=0.5, label='Reduced')\n","\n","plt.xlabel('x_values')\n","plt.ylabel('y_values')\n","plt.legend()\n","\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iPbOaVunaaRf","colab_type":"text"},"source":["Now let's restore \"original\" values by the reduced:"]},{"cell_type":"code","metadata":{"id":"eYUZ_E_caaRi","colab_type":"code","colab":{}},"source":["new_x = pca.restore()\n","print('Restored:')\n","print(new_x[:5])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zb1bvdpSaaRp","colab_type":"text"},"source":["**Expected Output**: \n","\n","<table>\n","    <tr>\n","       <td>\n","        Restored:\n","       </td>\n","       <td> [[11.51573657 41.48068378 39.47263947]\n"," [14.1995759  43.36459173 40.86399609]\n"," [19.41116905 38.09599585 36.27911059]\n"," [24.69623033 77.24494687 67.94674095]\n"," [27.07495924 74.84206341 65.85558128]]\n","    </tr>\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"hvtfeBnZaaRq","colab_type":"text"},"source":["Visualising of restored values:"]},{"cell_type":"code","metadata":{"id":"g6kZU-PYaaRr","colab_type":"code","colab":{}},"source":["from matplotlib import pyplot as plt\n","from mpl_toolkits.mplot3d import Axes3D\n","from mpl_toolkits.mplot3d import proj3d\n","\n","fig = plt.figure(figsize=(8,8))\n","ax = fig.add_subplot(111, projection='3d')\n","ax.plot(X.T[0,:], X.T[1,:], X.T[2,:], 'o', markersize=8, color='blue', alpha=0.5, label='Initial')\n","ax.plot(new_x.T[0,:], new_x.T[1,:], new_x.T[2,:], '^', markersize=8, alpha=0.5, color='red', label='Restored')\n","\n","ax.legend(loc='upper right')\n","\n","\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1-NEa2ipaaRx","colab_type":"text"},"source":["## 4 - Trying the Algorithm on Image"]},{"cell_type":"markdown","metadata":{"id":"J8rLkRUAaaRy","colab_type":"text"},"source":["Initializing the model for processing our data:"]},{"cell_type":"code","metadata":{"id":"AutIQu6waaRz","colab_type":"code","colab":{}},"source":["pca_img = PCA(img, 50, Eigendecomposition().covariance)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8-pIiKWpaaR4","colab_type":"code","colab":{}},"source":["reduced_img, reduced_eigenvects_img = pca_img.transform()\n","print('Reduced array:')\n","print(reduced_img.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AcVkXBWxaaR8","colab_type":"text"},"source":["**Expected Output**: \n","\n","<table >\n","    <tr>\n","       <td>\n","        Reduced array:\n","       </td>\n","       <td> (1280, 50)\n","    </tr>\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"NIvpWjTkaaR9","colab_type":"text"},"source":["Now let's restore the image and compare it with original:"]},{"cell_type":"code","metadata":{"id":"9hlAvHa3aaR_","colab_type":"code","colab":{}},"source":["restored_img = pca_img.restore()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zZ1qd0bgaaSE","colab_type":"code","colab":{}},"source":["imgplot = plt.imshow(img, cmap='gray')\n","print('Initial:')\n","plt.show()\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1Hb6O3dCaaSJ","colab_type":"code","colab":{}},"source":["imgplot = plt.imshow(restored_img, cmap='gray')\n","print('Restored:')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bKI29Qy-aaSN","colab_type":"text"},"source":["## 5 - Conclusion\n","\n","As we can see, our algorithm solves problem of reducing dimension of data well.\n","\n","**What's next:**\n","1. Try to experiment with different ways of eigendecomposition and values of k.\n","2. Compare the results you have obtained with the `sklearn.decomposition.PCA`.\n","3. Try this model in the wild! Select your favorite dataset [here](https://www.kaggle.com/datasets?sortBy=hottest&group=public&page=1&pageSize=20&size=small&filetype=all&license=all&tagids=13303) and play with it."]}]}